{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, genrename):\n",
    "    file = pd.read_csv(filename, sep='\\t')\n",
    "    file = file[file['genre'] == genrename]\n",
    "    data = [[file.iloc[i,0],file.iloc[i,1]] for i in range(len(file.index))]\n",
    "    file['label'] = file['label'].replace(['entailment', 'contradiction', 'neutral'], [0,1,2])\n",
    "    labels = file['label']\n",
    "#     file['genre'] = file['genre'].replace(['telephone', 'fiction', 'slate', 'government', 'travel'], [0,1,2,3,4])\n",
    "#     genres = file['genre']\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_1, val_targets_1 = load_data('mnli_val.tsv', 'telephone')\n",
    "val_data_2, val_targets_2 = load_data('mnli_val.tsv', 'fiction')\n",
    "val_data_3, val_targets_3 = load_data('mnli_val.tsv', 'slate')\n",
    "val_data_4, val_targets_4 = load_data('mnli_val.tsv', 'government')\n",
    "val_data_5, val_targets_5 = load_data('mnli_val.tsv', 'travel')\n",
    "\n",
    "# file = pd.read_csv('mnli_train.tsv', sep='\\t')\n",
    "# file = file[:10000]\n",
    "# file['label']= file['label'].replace(['entailment', 'contradiction', 'neutral'], [0,1,2])\n",
    "# file.iloc[5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tokens_1 = [[val_data_1[i][j].split() for j in range(2)] for i in range(len(val_data_1))]\n",
    "val_data_tokens_2 = [[val_data_2[i][j].split() for j in range(2)] for i in range(len(val_data_2))]\n",
    "val_data_tokens_3 = [[val_data_3[i][j].split() for j in range(2)] for i in range(len(val_data_3))]\n",
    "val_data_tokens_4 = [[val_data_4[i][j].split() for j in range(2)] for i in range(len(val_data_4))]\n",
    "val_data_tokens_5 = [[val_data_5[i][j].split() for j in range(2)] for i in range(len(val_data_5))]\n",
    "\n",
    "\n",
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    lines = []\n",
    "    for i in range(50001):\n",
    "        line = f.readline()\n",
    "        v = line.split()\n",
    "        for j in range(1,len(v)):\n",
    "            v[j] = float(v[j])\n",
    "        lines.append(v)\n",
    "        \n",
    "lines.remove(lines[0])  \n",
    "\n",
    "word_dict = {}\n",
    "word_dict['PAD'] = 0\n",
    "word_dict['UNK'] = 1\n",
    "\n",
    "embed = [[0 for i in range(300)],[0 for i in range(300)]]\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    word_dict['PAD'] = 0\n",
    "    word_dict['UNK'] = 1\n",
    "    word_dict[lines[i][0]] = i+2\n",
    "    embed.append(lines[i][1:])\n",
    "    \n",
    "embedding_matrix = np.matrix(embed)\n",
    "\n",
    "id2token = []\n",
    "for word in word_dict.keys():\n",
    "    id2token.append(word)\n",
    "token2id = word_dict\n",
    "\n",
    "UNK_IDX = 1\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        sublist = []\n",
    "        for i in range(2):\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens[i]]\n",
    "            sublist.append(index_list)\n",
    "        indices_data.append(sublist)\n",
    "    return indices_data\n",
    "\n",
    "val_data_indices_1 = token2index_dataset(val_data_tokens_1)\n",
    "val_data_indices_2 = token2index_dataset(val_data_tokens_2)\n",
    "val_data_indices_3 = token2index_dataset(val_data_tokens_3)\n",
    "val_data_indices_4 = token2index_dataset(val_data_tokens_4)\n",
    "val_data_indices_5 = token2index_dataset(val_data_tokens_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        self.target_list.index = range(len(self.target_list))\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx_1 = self.data_list[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx_2 = self.data_list[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx = [token_idx_1, token_idx_2]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, [len(token_idx_1), len(token_idx_2)], label]\n",
    "\n",
    "def MNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0][0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1][0])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[0][1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1][1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "#         sublist = ' '.join(padded_vec_2)\n",
    "#         data_list.append(padded_vec_1)\n",
    "#         data_list.append(padded_vec_2)\n",
    "        data_list.append(list(padded_vec_1) + list(padded_vec_2))\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "\n",
    "val_dataset_1 = MNLIDataset(val_data_indices_1, val_targets_1)\n",
    "val_dataset_2 = MNLIDataset(val_data_indices_2, val_targets_2)\n",
    "val_dataset_3 = MNLIDataset(val_data_indices_3, val_targets_3)\n",
    "val_dataset_4 = MNLIDataset(val_data_indices_4, val_targets_4)\n",
    "val_dataset_5 = MNLIDataset(val_data_indices_5, val_targets_5)\n",
    "\n",
    "val_loader_1 = torch.utils.data.DataLoader(dataset=val_dataset_1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader_2 = torch.utils.data.DataLoader(dataset=val_dataset_2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader_3 = torch.utils.data.DataLoader(dataset=val_dataset_3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader_4 = torch.utils.data.DataLoader(dataset=val_dataset_4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_loader_5 = torch.utils.data.DataLoader(dataset=val_dataset_5, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "#         print(model(data_batch, lengths_batch))\n",
    "#         print(outputs)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(embed)\n",
    "\n",
    "class Temp(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(Temp, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "#         self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)#creating RNN in pytorch\n",
    "        self.bi_gru1 = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True,bidirectional=True)\n",
    "        self.bi_gru2 = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True,bidirectional=True)\n",
    "\n",
    "        # 2 FC layers\n",
    "        self.linear1 = nn.Linear(4 * hidden_size, 100)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(100, 3)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len = x.size()   \n",
    "        #main part of rnn\n",
    "        self.hidden1 = self.init_hidden(batch_size) # old size = batch_size \n",
    "        self.hidden2 = self.init_hidden(batch_size) # old size = batch_size \n",
    "\n",
    "        #######################################################\n",
    "        # code here\n",
    "        \n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(x)\n",
    "        embed.detach()\n",
    "        # sort the sequence\n",
    "        sorted_seq_lengths1, indices1 = torch.sort(lengths[:, 0], descending=True)\n",
    "        sorted_seq_lengths2, indices2 = torch.sort(lengths[:, 1], descending=True)\n",
    "        \n",
    "        embed_1 = embed[:, :MAX_SENTENCE_LENGTH, :][indices1]\n",
    "        embed_2 = embed[:, MAX_SENTENCE_LENGTH:, :][indices2]\n",
    "        \n",
    "        \n",
    "        embed_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, \n",
    "                                                          sorted_seq_lengths1.numpy(), \n",
    "                                                          batch_first=True)\n",
    "        \n",
    "        embed_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, \n",
    "                                                          sorted_seq_lengths2.numpy(), \n",
    "                                                          batch_first=True)\n",
    "\n",
    "        _, desorted_indices1 = torch.sort(indices1, descending=False)\n",
    "        _, desorted_indices2 = torch.sort(indices2, descending=False)\n",
    "        \n",
    "        #bi-directional GRU\n",
    "        bi_output1, self.hidden1 = self.bi_gru1(embed_1,self.hidden1)\n",
    "        bi_output2, self.hidden2 = self.bi_gru2(embed_2,self.hidden2)\n",
    "        # rearrange output into correct order\n",
    "#         print(self.hidden1.size())\n",
    "        self.hidden1 = self.hidden1[:, desorted_indices1, :]\n",
    "        self.hidden2 = self.hidden2[:, desorted_indices2, :]\n",
    "\n",
    "        \n",
    "        bi_gru_out = torch.cat((torch.cat([self.hidden1[0], self.hidden1[1]], dim=-1), \n",
    "                                torch.cat([self.hidden2[0], self.hidden2[1]], dim=-1)), dim=1)\n",
    "\n",
    "        # FC layers\n",
    "        rnn_out = self.linear1(bi_gru_out)\n",
    "        activate = self.activation(rnn_out)\n",
    "        logits = self.linear2(activate)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.FloatTensor(embed)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size, ks):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "#         self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=ks, padding=int((ks-1)/2))\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=ks, padding=int((ks-1)/2))\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(emb_size, hidden_size, kernel_size=ks, padding=int((ks-1)/2))\n",
    "        self.conv4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=ks, padding=int((ks-1)/2))\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=MAX_SENTENCE_LENGTH)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size*2, 128)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        embed = self.dropout(embed)\n",
    "        embed_1 = embed[:, :MAX_SENTENCE_LENGTH, :] #100,50,300\n",
    "        embed_2 = embed[:, MAX_SENTENCE_LENGTH:, :]\n",
    "\n",
    "        hidden_1 = self.conv1(embed_1.transpose(1,2)).transpose(1, 2)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, MAX_SENTENCE_LENGTH, hidden_1.size(-1))\n",
    "        \n",
    "#         print(hidden_1.size())\n",
    "        hidden_1 = self.conv2(hidden_1.transpose(1,2)).transpose(1,2)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, MAX_SENTENCE_LENGTH, hidden_1.size(-1))\n",
    "        \n",
    "        hidden_2 = self.conv3(embed_2.transpose(1,2)).transpose(1, 2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, MAX_SENTENCE_LENGTH, hidden_2.size(-1))\n",
    "\n",
    "        hidden_2 = self.conv4(hidden_2.transpose(1,2)).transpose(1,2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, MAX_SENTENCE_LENGTH, hidden_2.size(-1))\n",
    "\n",
    "        # max pool\n",
    "        hidden_1 = self.maxpool(hidden_1.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden_1 = torch.squeeze(hidden_1, dim=1)\n",
    "        hidden_2 = self.maxpool(hidden_2.transpose(1, 2)).transpose(1, 2)\n",
    "        hidden_2 = torch.squeeze(hidden_2, dim=1)\n",
    "        \n",
    "        hidden = torch.cat((hidden_1, hidden_2), dim=1)\n",
    "#         print(hidden.size())\n",
    "        \n",
    "        hidden = self.linear1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.activation(hidden)\n",
    "        logits = self.linear2(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc for “telephone” = 46.865671641791046 \n",
      "Test Acc for “fiction” = 46.4321608040201 \n",
      "Test Acc for “slate” = 42.61477045908184 \n",
      "Test Acc for “government” = 47.93307086614173 \n",
      "Test Acc for “travel” = 45.21384928716904 \n"
     ]
    }
   ],
   "source": [
    "model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "# model.eval()\n",
    "\n",
    "print(\"Test Acc for “telephone” = {} \".format(test_model(val_loader_1, model)))\n",
    "print(\"Test Acc for “fiction” = {} \".format(test_model(val_loader_2, model)))\n",
    "print(\"Test Acc for “slate” = {} \".format(test_model(val_loader_3, model)))\n",
    "print(\"Test Acc for “government” = {} \".format(test_model(val_loader_4, model)))\n",
    "print(\"Test Acc for “travel” = {} \".format(test_model(val_loader_5, model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CNN(emb_size=300, hidden_size=500, num_layers=2, num_classes=3, vocab_size=len(id2token), ks=3)\n",
    "model2.load_state_dict(torch.load('cnnmodel.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc for “telephone” = 43.681592039801 \n",
      "Test Acc for “fiction” = 42.814070351758794 \n",
      "Test Acc for “slate” = 43.11377245508982 \n",
      "Test Acc for “government” = 42.42125984251968 \n",
      "Test Acc for “travel” = 44.19551934826884 \n"
     ]
    }
   ],
   "source": [
    "print(\"Test Acc for “telephone” = {} \".format(test_model(val_loader_1, model2)))\n",
    "print(\"Test Acc for “fiction” = {} \".format(test_model(val_loader_2, model2)))\n",
    "print(\"Test Acc for “slate” = {} \".format(test_model(val_loader_3, model2)))\n",
    "print(\"Test Acc for “government” = {} \".format(test_model(val_loader_4, model2)))\n",
    "print(\"Test Acc for “travel” = {} \".format(test_model(val_loader_5, model2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4270"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1, train_targets_1 = load_data('mnli_train.tsv', 'telephone')\n",
    "train_data_2, train_targets_2 = load_data('mnli_train.tsv', 'fiction')\n",
    "train_data_3, train_targets_3 = load_data('mnli_train.tsv', 'slate')\n",
    "train_data_4, train_targets_4 = load_data('mnli_train.tsv', 'government')\n",
    "train_data_5, train_targets_5 = load_data('mnli_train.tsv', 'travel')\n",
    "\n",
    "train_data_tokens_1 = [[train_data_1[i][j].split() for j in range(2)] for i in range(len(train_data_1))]\n",
    "train_data_tokens_2 = [[train_data_2[i][j].split() for j in range(2)] for i in range(len(train_data_2))]\n",
    "train_data_tokens_3 = [[train_data_3[i][j].split() for j in range(2)] for i in range(len(train_data_3))]\n",
    "train_data_tokens_4 = [[train_data_4[i][j].split() for j in range(2)] for i in range(len(train_data_4))]\n",
    "train_data_tokens_5 = [[train_data_5[i][j].split() for j in range(2)] for i in range(len(train_data_5))]\n",
    "\n",
    "train_data_indices_1 = token2index_dataset(train_data_tokens_1)\n",
    "train_data_indices_2 = token2index_dataset(train_data_tokens_2)\n",
    "train_data_indices_3 = token2index_dataset(train_data_tokens_3)\n",
    "train_data_indices_4 = token2index_dataset(train_data_tokens_4)\n",
    "train_data_indices_5 = token2index_dataset(train_data_tokens_5)\n",
    "\n",
    "train_dataset_1 = MNLIDataset(train_data_indices_1, train_targets_1)\n",
    "train_dataset_2 = MNLIDataset(train_data_indices_2, train_targets_2)\n",
    "train_dataset_3 = MNLIDataset(train_data_indices_3, train_targets_3)\n",
    "train_dataset_4 = MNLIDataset(train_data_indices_4, train_targets_4)\n",
    "train_dataset_5 = MNLIDataset(train_data_indices_5, train_targets_5)\n",
    "\n",
    "train_loader_1 = torch.utils.data.DataLoader(dataset=train_dataset_1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "train_loader_2 = torch.utils.data.DataLoader(dataset=train_dataset_2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "train_loader_3 = torch.utils.data.DataLoader(dataset=train_dataset_3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "train_loader_4 = torch.utils.data.DataLoader(dataset=train_dataset_4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "train_loader_5 = torch.utils.data.DataLoader(dataset=train_dataset_5, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on telephone model\n",
      "For genre : telephone, accuracy = 51.74129353233831\n",
      "For genre : fiction, accuracy = 50.050251256281406\n",
      "For genre : slate, accuracy = 47.10578842315369\n",
      "For genre : government, accuracy = 46.94881889763779\n",
      "For genre : travel, accuracy = 49.89816700610998\n"
     ]
    }
   ],
   "source": [
    "# model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "# model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0003\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "parameters = itertools.filterfalse(lambda p: p.requires_grad == False, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model\n",
    "total_step = len(train_loader_1)\n",
    "\n",
    "\n",
    "loss_l = []\n",
    "acc_l = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data,lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if i % 100 == 0:\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader_1, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader_1), val_acc))\n",
    "#             acc_l.append(val_acc)\n",
    "\n",
    "print('Tuning on telephone model')\n",
    "val_acc_1 = test_model(val_loader_1, model)\n",
    "val_acc_2 = test_model(val_loader_2, model)\n",
    "val_acc_3 = test_model(val_loader_3, model)\n",
    "val_acc_4 = test_model(val_loader_4, model)\n",
    "val_acc_5 = test_model(val_loader_5, model)\n",
    "print('For genre : telephone, accuracy = ' + str(val_acc_1))\n",
    "print('For genre : fiction, accuracy = ' + str(val_acc_2))\n",
    "print('For genre : slate, accuracy = ' + str(val_acc_3))\n",
    "print('For genre : government, accuracy = ' + str(val_acc_4))\n",
    "print('For genre : travel, accuracy = ' + str(val_acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on fiction model\n",
      "For genre : telephone, accuracy = 49.75124378109453\n",
      "For genre : fiction, accuracy = 50.25125628140704\n",
      "For genre : slate, accuracy = 46.20758483033932\n",
      "For genre : government, accuracy = 47.539370078740156\n",
      "For genre : travel, accuracy = 47.14867617107943\n"
     ]
    }
   ],
   "source": [
    "model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0003\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "parameters = itertools.filterfalse(lambda p: p.requires_grad == False, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model\n",
    "total_step = len(train_loader_2)\n",
    "\n",
    "\n",
    "loss_l = []\n",
    "acc_l = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_2):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data,lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if i % 100 == 0:\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader_1, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader_1), val_acc))\n",
    "#             acc_l.append(val_acc)\n",
    "\n",
    "print('Tuning on fiction model')\n",
    "val_acc_1 = test_model(val_loader_1, model)\n",
    "val_acc_2 = test_model(val_loader_2, model)\n",
    "val_acc_3 = test_model(val_loader_3, model)\n",
    "val_acc_4 = test_model(val_loader_4, model)\n",
    "val_acc_5 = test_model(val_loader_5, model)\n",
    "print('For genre : telephone, accuracy = ' + str(val_acc_1))\n",
    "print('For genre : fiction, accuracy = ' + str(val_acc_2))\n",
    "print('For genre : slate, accuracy = ' + str(val_acc_3))\n",
    "print('For genre : government, accuracy = ' + str(val_acc_4))\n",
    "print('For genre : travel, accuracy = ' + str(val_acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on slate model\n",
      "For genre : telephone, accuracy = 48.159203980099505\n",
      "For genre : fiction, accuracy = 49.14572864321608\n",
      "For genre : slate, accuracy = 47.405189620758485\n",
      "For genre : government, accuracy = 50.88582677165354\n",
      "For genre : travel, accuracy = 47.55600814663951\n"
     ]
    }
   ],
   "source": [
    "# model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "# model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0003\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "parameters = itertools.filterfalse(lambda p: p.requires_grad == False, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model\n",
    "total_step = len(train_loader_3)\n",
    "\n",
    "\n",
    "loss_l = []\n",
    "acc_l = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_3):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data,lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if i % 100 == 0:\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader_1, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader_1), val_acc))\n",
    "#             acc_l.append(val_acc)\n",
    "\n",
    "print('Tuning on slate model')\n",
    "val_acc_1 = test_model(val_loader_1, model)\n",
    "val_acc_2 = test_model(val_loader_2, model)\n",
    "val_acc_3 = test_model(val_loader_3, model)\n",
    "val_acc_4 = test_model(val_loader_4, model)\n",
    "val_acc_5 = test_model(val_loader_5, model)\n",
    "print('For genre : telephone, accuracy = ' + str(val_acc_1))\n",
    "print('For genre : fiction, accuracy = ' + str(val_acc_2))\n",
    "print('For genre : slate, accuracy = ' + str(val_acc_3))\n",
    "print('For genre : government, accuracy = ' + str(val_acc_4))\n",
    "print('For genre : travel, accuracy = ' + str(val_acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on government model\n",
      "For genre : telephone, accuracy = 46.666666666666664\n",
      "For genre : fiction, accuracy = 48.54271356783919\n",
      "For genre : slate, accuracy = 47.405189620758485\n",
      "For genre : government, accuracy = 53.44488188976378\n",
      "For genre : travel, accuracy = 48.37067209775967\n"
     ]
    }
   ],
   "source": [
    "model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0003\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "parameters = itertools.filterfalse(lambda p: p.requires_grad == False, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model\n",
    "total_step = len(train_loader_4)\n",
    "\n",
    "\n",
    "loss_l = []\n",
    "acc_l = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_4):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data,lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if i % 100 == 0:\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader_1, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader_1), val_acc))\n",
    "#             acc_l.append(val_acc)\n",
    "\n",
    "print('Tuning on government model')\n",
    "val_acc_1 = test_model(val_loader_1, model)\n",
    "val_acc_2 = test_model(val_loader_2, model)\n",
    "val_acc_3 = test_model(val_loader_3, model)\n",
    "val_acc_4 = test_model(val_loader_4, model)\n",
    "val_acc_5 = test_model(val_loader_5, model)\n",
    "print('For genre : telephone, accuracy = ' + str(val_acc_1))\n",
    "print('For genre : fiction, accuracy = ' + str(val_acc_2))\n",
    "print('For genre : slate, accuracy = ' + str(val_acc_3))\n",
    "print('For genre : government, accuracy = ' + str(val_acc_4))\n",
    "print('For genre : travel, accuracy = ' + str(val_acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning on travel model\n",
      "For genre : telephone, accuracy = 49.55223880597015\n",
      "For genre : fiction, accuracy = 48.14070351758794\n",
      "For genre : slate, accuracy = 45.10978043912176\n",
      "For genre : government, accuracy = 50.88582677165354\n",
      "For genre : travel, accuracy = 53.462321792260695\n"
     ]
    }
   ],
   "source": [
    "model = Temp(emb_size=300, hidden_size=600, num_layers=1, num_classes=3, vocab_size=len(id2token))\n",
    "model.load_state_dict(torch.load('rnnmodel.pt', map_location='cpu'))\n",
    "num_epochs = 3\n",
    "learning_rate = 0.0003\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "parameters = itertools.filterfalse(lambda p: p.requires_grad == False, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "# Train the model\n",
    "total_step = len(train_loader_5)\n",
    "\n",
    "\n",
    "loss_l = []\n",
    "acc_l = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader_5):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data,lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        if i % 100 == 0:\n",
    "            loss_l.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "#         if i > 0 and i % 100 == 0:\n",
    "#             # validate\n",
    "#             val_acc = test_model(val_loader_1, model)\n",
    "#             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                        epoch+1, num_epochs, i+1, len(train_loader_1), val_acc))\n",
    "#             acc_l.append(val_acc)\n",
    "\n",
    "print('Tuning on travel model')\n",
    "val_acc_1 = test_model(val_loader_1, model)\n",
    "val_acc_2 = test_model(val_loader_2, model)\n",
    "val_acc_3 = test_model(val_loader_3, model)\n",
    "val_acc_4 = test_model(val_loader_4, model)\n",
    "val_acc_5 = test_model(val_loader_5, model)\n",
    "print('For genre : telephone, accuracy = ' + str(val_acc_1))\n",
    "print('For genre : fiction, accuracy = ' + str(val_acc_2))\n",
    "print('For genre : slate, accuracy = ' + str(val_acc_3))\n",
    "print('For genre : government, accuracy = ' + str(val_acc_4))\n",
    "print('For genre : travel, accuracy = ' + str(val_acc_5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
